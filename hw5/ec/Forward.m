function [output, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer pre activations in 'act_a', and the hidden layer post
% activations in 'act_h'.
% act_a is the pre-activation and act_h is the post-activation


act_a{1}=W{1}*X+b{1};
% act_h{1}=1./(1+exp(act_a{1}));

for i=2:length(W)
    act_h{i-1}=1./(1+exp(-act_a{i-1})); % sigmoid
    act_a{i}=W{i}*act_h{i-1}+b{i};
    
end

output=exp(act_a{end})./repmat(sum(exp(act_a{end})),size(act_a{end},1),1);
